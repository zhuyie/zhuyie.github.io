<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>zhuyie&#39;s blog</title>
    <link>https://zhuyie.github.io/</link>
    <description>Recent content on zhuyie&#39;s blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>© zhuyie</copyright>
    <lastBuildDate>Mon, 30 Jan 2023 22:15:36 +0800</lastBuildDate>
    <atom:link href="https://zhuyie.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>BLE蓝牙键盘在macOS/Windows双启动场景下的配对</title>
      <link>https://zhuyie.github.io/posts/ble-paring-on-dualboot-macos-windows/</link>
      <pubDate>Mon, 30 Jan 2023 22:15:36 +0800</pubDate>
      <guid>https://zhuyie.github.io/posts/ble-paring-on-dualboot-macos-windows/</guid>
      <description>起因 最近两把老键盘陆续挂掉，于是先后入手了两把新的：NIZ宁芝的X87EC三模1 和 Keydous的NJ81双模2。用下来都还可以，特别是两把的产地都是本地海边小渔村，感觉就更亲切了。&#xA;然而好景不长。我的电脑是 MacBookPro (16-inch, 2019)，平时主要用 macOS (Catalina 10.15.7)，偶尔会要切到 BootCamp 的 Windows 10 中做一些测试的事情。这两把键盘都支持3套蓝牙Profile，也就是可以保存3套蓝牙配对信息并快速切换连接。在macOS下我都使用键盘的BT1来配对使用；当reboot到Windows时，按我之前使用其它蓝牙外设（例如Logitech MX Master 3）的经验，我应该切换到BT2在Windows中再进行一次配对，然后就视当前启动的是哪个OS，就切到对应的Profile来连接使用即可。这套用法在两把新键盘这边卡壳了：当我在Windows下用BT2配对好正常使用后，再reboot回macOS，发现BT1已经无法连接；重新用BT1再进行一次配对，然后reboot到Windows，BT2也无法连接&amp;hellip; 也就是每次切换OS想用键盘都需要重新配对～～～&#xA;简单分析一下。电脑是同一台，无论是哪一个OS，电脑端的BTADDR是不变的。蓝牙键盘与电脑配对，是一个协商通讯密钥的过程，每一次配对应该都会生成不同的密钥。这两把键盘在BT2配对后，会影响到BT1，可能是因为这两组配对中，对端的BTADDR相同。猜测键盘为了实现某些自动连接切换的功能，内部以REMOTE_BTADDR为Key进行配对数据的存储，因此后一次的配对（生成了新的密钥）会影响到前一次的配对。&#xA;双启动场景蓝牙配对问题 能支持多套蓝牙配对的设备是近些年才多起来的，那么之前的老蓝牙设备在双启动场景下如何配对使用呢？搜索了一下，被这个问题困扰的人还真不少：&#xA;Bluetooth Pairing on Dual Boot of Windows &amp;amp; Linux Mint/Ubuntu Pairing bluetooth devices (keyboard or mouse) in dual boot with Linux Ubuntu and Windows 10 解决方案：Win10和Linux双系统配对蓝牙设备 特别的，这一篇刚好讲的就是macOS和BootCamp的场景：macOS 与 BootCamp 双系统共用同一蓝牙设备指引。&#xA;那么回到之前的问题，是不是我固定用BT1，然后按前述文章进行系统配置的调整，就能够愉快的玩耍了呢？&#xA;仔细看了这几篇文章，简单小结一下：&#xA;蓝牙设备与电脑配对时，会动态生成一组LinkKey用于后继的连接与通讯，这个数据会被保存在蓝牙设备的Device memory和电脑OS的配置文件/注册表中； Windows中的LinkKeys保存在注册表的HKLM\CurrentControlSet\Services\BTHPORT\Parameters\Keys\{PC_BTADDR}\下，以蓝牙设备的地址为Name。这个注册表路径的拥有者是SYSTEM账户，普通用户或Administrator用户是看不到的，但可以使用PsExec工具以SYSTEM账户身份启动regedit.exe来查看/修改； macOS的LinkKeys保存在配置文件/private/var/root/Library/Preferences/com.apple.bluetoothd.plist中，可以用sudo来操作； 将蓝牙设备先与OS1进行配对，然后切换到OS2再次配对，再将OS2中此蓝牙设备所对应的LinkKey数据提取出来（一段16进制数据），回到OS1，将对应LinkKey修改为所提取的值，重启； 完工！ 这么简单，那就动手吧。。。然而（总是会有个然而的）惊喜的发现，无论在macOS下还是Windows下，对应位置的LinkKeys中都没有发现这两把键盘对应的数据：&#xA;~ sudo defaults read /private/var/root/Library/Preferences/com.apple.bluetoothd.plist { DaemonIDSPairedDevices = ( .</description>
    </item>
    <item>
      <title>Happy New Year</title>
      <link>https://zhuyie.github.io/posts/happy-new-year-2023/</link>
      <pubDate>Sat, 31 Dec 2022 09:20:52 +0800</pubDate>
      <guid>https://zhuyie.github.io/posts/happy-new-year-2023/</guid>
      <description>2022的最后一天。虽然计划的事情好多没有完成，但所幸工作顺利，家人身体健康。&#xA;新年快乐 友谊万岁 </description>
    </item>
    <item>
      <title>bsdiff源码解析</title>
      <link>https://zhuyie.github.io/posts/bsdiff-annotated/</link>
      <pubDate>Wed, 16 Feb 2022 09:55:46 +0800</pubDate>
      <guid>https://zhuyie.github.io/posts/bsdiff-annotated/</guid>
      <description>简介 bsdiff是一种二进制增量编码（delta encoding1）算法。它由Colin Percival发明及开源，被广泛应用于各类二进制数据（特别是可执行程序）的增量更新（delta updates）场景中，例如Android使用bsdiff来减小OTA更新数据包的尺寸2。&#xA;bsdiff的官网在https://www.daemonology.net/bsdiff/，它由bsdiff和bspatch两个模块组成，前者负责生成新版本相对于旧版本的增量数据，后者负责基于旧版本和增量数据还原出新版本。bsdiff目前的最新版本是（很多年发布的）4.3。&#xA;作为一个典型的博士生作品，bsdiff源码的工程化水平相当一般，代码结构和风格、变量命名等都比较马虎。为了便于理解，这里结合4.3的源码进行一个详细点的分析。&#xA;原理 Colin Percival有一篇3页的小论文Naive differences of executable code，描述了bsdiff算法的基本原理和思路。简单来说，源代码中的少量修改（例如增/删/改几行代码）反映到编译后的可执行文件上会出现修改量大幅度放大的情况。这主要是因为可执行文件中会记录很多地址类数据（例如分支语句的跳转目标），大量的地址会因为少量源代码的修改而出现位移。此时如果按照传统的copy-and-insert方法来构造binary patch，效率会相当低下。在作者的实验中，一个500kB左右的可执行程序（按常规经验至少对应10万行以上的C源码）的源码中进行1行修改，会产生约50kB的patch文件。&#xA;对这个问题的一种可能的处理方法是，在算法上加入对可执行文件的格式和内部结构的理解。例如通过反汇编的方式将可执行文件还原为类汇编源码的形态，再对新旧两个版本的源码进行增量编码。这种方法的问题是算法复杂且和特定平台相关。Google的Courgette就用了这种思路。&#xA;bsdiff的目标是以平台无关的方式解决上述问题。它基于两个重要观察：&#xA;在可执行文件的与源码修改无直接对应关系的区域，所产生的差异是相当稀疏的（sparse）：(a)地址位移仅影响到部分的编译后指令；(b)地址位移的幅度一般较小，很多时候只影响到地址字的最后1～2个字节； 数据和代码倾向于按整块的方式移动，通常存在很多“块”，其中的地址位移都按相同的差值发生了变化； 由此可以想到，如果找出某个可执行程序的两个版本中那些对应源码没有改动的区域，对这些区域按字节计算差值（bytewise differences），其结果应该会存在很多0（因为内容相同），即使不为0也经常会出现高频重复值。换句话说，这些差值结果具有很高的可压缩性（highly compressible）。&#xA;源码解析 patchfile format 这部分在bspatch.c中有较详细的注释：&#xA;/* File format: 0&#x9;8&#x9;&amp;#34;BSDIFF40&amp;#34; 8&#x9;8&#x9;X 16&#x9;8&#x9;Y 24&#x9;8&#x9;sizeof(newfile) 32&#x9;X&#x9;bzip2(control block) 32+X&#x9;Y&#x9;bzip2(diff block) 32+X+Y&#x9;???&#x9;bzip2(extra block) with control block a set of triples (x,y,z) meaning &amp;#34;add x bytes from oldfile to x bytes from the diff block; copy y bytes from the extra block; seek forwards in oldfile by z bytes&amp;#34;.</description>
    </item>
    <item>
      <title>从Base16到Base85, 谈谈Binary-to-text编码</title>
      <link>https://zhuyie.github.io/posts/binary-to-text-encoding/</link>
      <pubDate>Mon, 09 Aug 2021 09:56:58 +0800</pubDate>
      <guid>https://zhuyie.github.io/posts/binary-to-text-encoding/</guid>
      <description>起因 最近在看ImGui的代码，其中有个将字体数据嵌入到源码的写法1引起了我的注意，它采用了一种之前没接触过的编码方式：Base85。从这里开始，对相关的领域进行了一次简单的梳理。&#xA;什么是Binary-to-text编码2？ 顾名思义，Binary-to-text编码是一种将二进制数据转换为文本形式的编码方式。更精确的说，它是一种将二进制数据转换为由 “可打印字符” 组成的文本流形式的编码方式。&#xA;为什么要存在这个东西？主要是为了解决如下两类问题：&#xA;某些传输信道或介质不支持二进制数据，例如E-Mail或NNTP； 某些传输信道或介质不能正确的处理8-bit字符编码（或者说：not 8-bit clean）； 看下这个编码的两端，一边是二进制数据，一边是文本。&#xA;二进制数据，大家通常的理解是一个字节流，同时每1个字节由8个比特组成。稍微延伸一下，Quora上这个问题比较详细的解释了Why is one byte formed by 8 bits?&#xA;文本，在这个上下文中，特指的是ASCII编码中的printable characters。ASCII编码是一个7-bit的编码，最多可表示128个唯一字符。其中控制字符33个（0x0-0x1F, 0x7F），可打印字符95个（0x20-0x7E）。&#xA;一个重要的问题是，在现代二进制计算机中，文本（或字符）在实现上也是基于字节来存储，也就是虽然单个ASCII字符只需要用7-bit来描述，在具体实现中1个ASCII字符会被存储为1个字节，也即占用8-bit。换句话说，从信息编码效率来看，以字节形式存储的文本，编码效率最高不会超过87.5% (7/8)，并随着编码中所实际使用的唯一字符数的降低而进一步降低。与之对应的，Binary-to-text编码的效率通常也会低于1，也就是编码后的文本串所占用的字节数通常要大于原始二进制数据所占的字节数。&#xA;回到ImGui将字体嵌入源码的情况：字体文件是二进制数据，而C++源码中的string literal当然无法支持二进制数据，因此这里使用了Base85将二进制数据编码为文本流，再嵌入到源码中。&#xA;Base16 Base16或者说是hexadecimal，应该是程序员们最熟悉的一种Binary-to-text编码了。其文本中的单个字符具有16种唯一值，能表示4-bit的信息（2^4 = 16），因此一个8-bit的字节刚好可以用两个字符来表示，编码效率为50%。&#xA;对应到文本字符的选择上，理论上可以从可打印字符中任意挑选16个来表示，Base16实际选择了最容易理解的0, 1, 2, &amp;hellip;, 9, a, b, c, d, e, f这16个字符来分别表示0 ~ 15，其中这6个字母大小写不敏感。&#xA;看个例子，开发人员常用的HexViewer类工具，是一种典型的Base16转换场景。例如用HexViewer打开一个.zip文件，可以看到其前4个字节的Magic word为：&amp;ldquo;PK␃␄&amp;rdquo;: Base64 Base64应该是使用最多的Binary-to-text编码，被广泛用于互联网环境，例如在HTML/CSS中嵌入二进制数据，在E-Mail消息中编码二进制附件文件等。其文本中的单个字符具有64种唯一值，能表示6-bit的信息（2^6 = 64），因此3个字节的二进制数据（3 x 8-bit）刚好可以用4个字符来表示（4 x 6-bit），编码效率为75%。&#xA;Why 64? 主要原因是：&#xA;Base为2的n次幂时，方便进行Base 2到Base 2^n的转换（n个bit为一组进行转换即可）； ASCII中共有95个可打印字符，而64是&amp;lt;=95的数中最大的2的n次幂数； 在文本字符子集的选择上，Base64稍有不同：大写字母A~Z表示0~25，小写字母a~z表示26~51，数字0~9表示52~61，+表示62，/表示63。此外，=字符用于将编码后文本长度padding到4的倍数。 网上有简易的Base64编码在线转换工具3，我们来看个例子： 输入文本中的前3个字节为&amp;quot;Hel&amp;quot;，对应的二进制为：01001000 01100101 01101100。 将之按6-bit为一组切分为4组，结果为：010010 000110 010101 101100，查表可知对应的Base64字符为：S G V s。</description>
    </item>
    <item>
      <title>一个测试CPU分支预测的示例程序</title>
      <link>https://zhuyie.github.io/posts/cpu-branch-prediction-demo/</link>
      <pubDate>Tue, 18 May 2021 11:26:35 +0800</pubDate>
      <guid>https://zhuyie.github.io/posts/cpu-branch-prediction-demo/</guid>
      <description>起因 最近在看一些CPU Microarchitecture相关的资料。其中的分支预测器1，虽然知道概念，但其对性能的影响程度没有清晰的概念。因此，设计了一个小测试程序2。&#xA;Test program #include &amp;lt;cstdio&amp;gt; #include &amp;lt;cstdlib&amp;gt; #include &amp;lt;vector&amp;gt; #include &amp;lt;random&amp;gt; #include &amp;lt;algorithm&amp;gt; #include &amp;lt;chrono&amp;gt; using namespace std::chrono; static int dummy[2]; int main(int argc, char* argv[]) { // parsing command line bool sort = false; if (argc &amp;gt; 1 &amp;amp;&amp;amp; strcmp(argv[1], &amp;#34;1&amp;#34;) == 0) { sort = true; } // prepare test data (uniformly distributed random values) const int arraySize = 5000; std::vector&amp;lt;int&amp;gt; data; data.resize(arraySize); std::random_device rd; std::mt19937 gen(rd()); std::uniform_int_distribution&amp;lt;&amp;gt; dis(0, 256); for (int i = 0; i &amp;lt; arraySize; i++) { data[i] = dis(gen); } // sort or not if (sort) { std::sort(data.</description>
    </item>
    <item>
      <title>CPU缓存伪共享(False Sharing)问题</title>
      <link>https://zhuyie.github.io/posts/cpu-cache-false-sharing/</link>
      <pubDate>Mon, 03 May 2021 13:06:16 +0800</pubDate>
      <guid>https://zhuyie.github.io/posts/cpu-cache-false-sharing/</guid>
      <description>起因 一个通过蒙特卡洛方法估算 π 值的测试程序，采用了多线程的方式，其代码大致如下：&#xA;#include &amp;#34;...&amp;#34; class RandomNumber { mt19937 mt19937_; // random number generator const float MT19937_FLOAT_MULTI = 2.3283064365386962890625e-10f; // (2^32-1)^-1 public: void seed(unsigned int seed) { mt19937_.seed(seed); } float operator() () { return mt19937_() * MT19937_FLOAT_MULTI; } }; void worker(int worker_id, int64_t samples, RandomNumber *rnd, int64_t *in) { rnd-&amp;gt;seed(worker_id); // seed the prng for (int64_t i = 0; i &amp;lt; samples; i++) { float x = (*rnd)(); float y = (*rnd)(); if (x*x + y*y &amp;lt;= 1) { (*in)++; } } } int main(int argc, char* argv[]) { int num_threads = 16; int64_t num_samples = 1000000000; // parsing command line arguments auto start = system_clock::now(); RandomNumber* rnd = new RandomNumber[num_threads]; int64_t* in_points = new int64_t[num_threads]; int64_t total_points = num_samples; int64_t circle_points = 0; vector&amp;lt;thread&amp;gt; threads; for (int i = 0; i &amp;lt; num_threads; i++) { int64_t samples = num_samples / num_threads; if (i == num_threads - 1) { samples += num_samples - samples * num_threads; } threads.</description>
    </item>
    <item>
      <title>在MacOS命令行程序中使用Metal</title>
      <link>https://zhuyie.github.io/posts/use-metal-in-command-line-app/</link>
      <pubDate>Mon, 19 Apr 2021 21:20:23 +0800</pubDate>
      <guid>https://zhuyie.github.io/posts/use-metal-in-command-line-app/</guid>
      <description>出错 最近在试验用GPU做计算，由于日常工作在MacBook Pro上，很自然想要试下Apple的Metal API。&#xA;官方有个示例程序Performing Calculations on a GPU，很容易懂。下载示例代码到本地构建运行，完全正常。这么简单的吗？&#xA;那就开始用到自己的程序中吧！因为要与CPU/OpenCL等版本进行横向对比，很自然的Metal版本也使用了command line app的形式。代码写完编译很顺利，试跑一下，结果不对。。。&#xA;作为老程序员，遇到这种事情很淡定。即使是测试用的小程序，我也加上了错误处理相关的代码（良好的编程素养！），那看一下错误输出，查一下文档，一般都能搞定。&#xA;出错的代码位置是：&#xA;MTLCompileOptions* compileOptions = [MTLCompileOptions new]; id&amp;lt;MTLLibrary&amp;gt; lib = [device newLibraryWithSource:source options:compileOptions error:&amp;amp;error]; if (lib == nil) { NSLog(@&amp;#34;Failed to create library: %@.&amp;#34;, error); return nil; } 而对应的错误输出为：&#xA;2021-04-19 22:00:10.546 estimate_pi_metal[3334:87426] Failed to create library: (null). 也就是：newLibraryWithSource执行失败返回了nil，与此同时error也为null。&#xA;官方文档 newLibraryWithSource官方文档上对Return Value的说明：&#xA;A new library object that contains the compiled source code or nil if an error occurred. 对error参数的说明：</description>
    </item>
    <item>
      <title>VC2019 std::generate_canonical 性能问题</title>
      <link>https://zhuyie.github.io/posts/cpp-generate-canonical-perf/</link>
      <pubDate>Mon, 19 Apr 2021 11:07:20 +0800</pubDate>
      <guid>https://zhuyie.github.io/posts/cpp-generate-canonical-perf/</guid>
      <description>起因 最近一个项目中需要生成一堆均匀分布的伪随机浮点数。在C++11中，随机数生成算法以及相关的工具函数进行了不小的扩充。对于我的需求，常规的实现方法是：&#xA;#include &amp;lt;random&amp;gt; #include &amp;lt;iostream&amp;gt; int main() { std::random_device rd; std::mt19937 gen(rd()); // Standard mersenne_twister_engine seeded with rd() std::uniform_real_distribution&amp;lt;float&amp;gt; dis(1.0f, 2.0f); for (int n = 0; n &amp;lt; 10; ++n) { // Use dis to transform the random unsigned int generated by gen into a // float in [1, 2). Each call to dis(gen) generates a new random float std::cout &amp;lt;&amp;lt; dis(gen) &amp;lt;&amp;lt; &amp;#39; &amp;#39;; } std::cout &amp;lt;&amp;lt; &amp;#39;\n&amp;#39;; return 0; } 其输出类似于：</description>
    </item>
  </channel>
</rss>
