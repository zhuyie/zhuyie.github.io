<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>zhuyie&#39;s blog</title>
    <link>https://zhuyie.github.io/</link>
    <description>Recent content on zhuyie&#39;s blog</description>
    <generator>Hugo -- gohugo.io</generator>
    <language>en-us</language>
    <copyright>© zhuyie</copyright>
    <lastBuildDate>Mon, 09 Aug 2021 09:56:58 +0800</lastBuildDate><atom:link href="https://zhuyie.github.io/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>从Base16到Base85, 谈谈Binary-to-text编码</title>
      <link>https://zhuyie.github.io/posts/binary-to-text-encoding/</link>
      <pubDate>Mon, 09 Aug 2021 09:56:58 +0800</pubDate>
      
      <guid>https://zhuyie.github.io/posts/binary-to-text-encoding/</guid>
      <description>起因 最近在看ImGui的代码，其中有个将字体数据嵌入到源码的写法1引起了我的注意，它采用了一种之前没接触过的编码方式：Base85。从这里开始，对相关的领域进行了一次简单的梳理。
什么是Binary-to-text编码2？ 顾名思义，Binary-to-text编码是一种将二进制数据转换为文本形式的编码方式。更精确的说，它是一种将二进制数据转换为由 “可打印字符” 组成的文本流形式的编码方式。
为什么要存在这个东西？主要是为了解决如下两类问题：
 某些传输信道或介质不支持二进制数据，例如E-Mail或NNTP； 某些传输信道或介质不能正确的处理8-bit字符编码（或者说：not 8-bit clean）；  看下这个编码的两端，一边是二进制数据，一边是文本。
二进制数据，大家通常的理解是一个字节流，同时每1个字节由8个比特组成。稍微延伸一下，Quora上这个问题比较详细的解释了Why is one byte formed by 8 bits?
文本，在这个上下文中，特指的是ASCII编码中的printable characters。ASCII编码是一个7-bit的编码，最多可表示128个唯一字符。其中控制字符33个（0x0-0x1F, 0x7F），可打印字符95个（0x20-0x7E）。
一个重要的问题是，在现代二进制计算机中，文本（或字符）在实现上也是基于字节来存储，也就是虽然单个ASCII字符只需要用7-bit来描述，在具体实现中1个ASCII字符会被存储为1个字节，也即占用8-bit。换句话说，从信息编码效率来看，以字节形式存储的文本，编码效率最高不会超过87.5% (7/8)，并随着编码中所实际使用的唯一字符数的降低而进一步降低。与之对应的，Binary-to-text编码的效率通常也会低于1，也就是编码后的文本串所占用的字节数通常要大于原始二进制数据所占的字节数。
回到ImGui将字体嵌入源码的情况：字体文件是二进制数据，而C++源码中的string literal当然无法支持二进制数据，因此这里使用了Base85将二进制数据编码为文本流，再嵌入到源码中。
Base16 Base16或者说是hexadecimal，应该是程序员们最熟悉的一种Binary-to-text编码了。其文本中的单个字符具有16种唯一值，能表示4-bit的信息（2^4 = 16），因此一个8-bit的字节刚好可以用两个字符来表示，编码效率为50%。
对应到文本字符的选择上，理论上可以从可打印字符中任意挑选16个来表示，Base16实际选择了最容易理解的0, 1, 2, &amp;hellip;, 9, a, b, c, d, e, f这16个字符来分别表示0 ~ 15，其中这6个字母大小写不敏感。
看个例子，开发人员常用的HexViewer类工具，是一种典型的Base16转换场景。例如用HexViewer打开一个.zip文件，可以看到其前4个字节的Magic word为：&amp;ldquo;PK␃␄&amp;rdquo;: Base64 Base64应该是使用最多的Binary-to-text编码，被广泛用于互联网环境，例如在HTML/CSS中嵌入二进制数据，在E-Mail消息中编码二进制附件文件等。其文本中的单个字符具有64种唯一值，能表示6-bit的信息（2^6 = 64），因此3个字节的二进制数据（3 x 8-bit）刚好可以用4个字符来表示（4 x 6-bit），编码效率为75%。
Why 64? 主要原因是：
 Base为2的n次幂时，方便进行Base 2到Base 2^n的转换（n个bit为一组进行转换即可）； ASCII中共有95个可打印字符，而64是&amp;lt;=95的数中最大的2的n次幂数；  在文本字符子集的选择上，Base64稍有不同：大写字母A~Z表示0~25，小写字母a~z表示26~51，数字0~9表示52~61，+表示62，/表示63。此外，=字符用于将编码后文本长度padding到4的倍数。 网上有简易的Base64编码在线转换工具3，我们来看个例子： 输入文本中的前3个字节为&amp;quot;Hel&amp;quot;，对应的二进制为：01001000 01100101 01101100。 将之按6-bit为一组切分为4组，结果为：010010 000110 010101 101100，查表可知对应的Base64字符为：S G V s。</description>
    </item>
    
    <item>
      <title>一个测试CPU分支预测的示例程序</title>
      <link>https://zhuyie.github.io/posts/cpu-branch-prediction-demo/</link>
      <pubDate>Tue, 18 May 2021 11:26:35 +0800</pubDate>
      
      <guid>https://zhuyie.github.io/posts/cpu-branch-prediction-demo/</guid>
      <description>起因 最近在看一些CPU Microarchitecture相关的资料。其中的分支预测器1，虽然知道概念，但其对性能的影响程度没有清晰的概念。因此，设计了一个小测试程序2。
Test program #include &amp;lt;cstdio&amp;gt;#include &amp;lt;cstdlib&amp;gt;#include &amp;lt;vector&amp;gt;#include &amp;lt;random&amp;gt;#include &amp;lt;algorithm&amp;gt;#include &amp;lt;chrono&amp;gt;using namespace std::chrono; static int dummy[2]; int main(int argc, char* argv[]) { // parsing command line  bool sort = false; if (argc &amp;gt; 1 &amp;amp;&amp;amp; strcmp(argv[1], &amp;#34;1&amp;#34;) == 0) { sort = true; } // prepare test data (uniformly distributed random values)  const int arraySize = 5000; std::vector&amp;lt;int&amp;gt; data; data.resize(arraySize); std::random_device rd; std::mt19937 gen(rd()); std::uniform_int_distribution&amp;lt;&amp;gt; dis(0, 256); for (int i = 0; i &amp;lt; arraySize; i++) { data[i] = dis(gen); } // sort or not  if (sort) { std::sort(data.</description>
    </item>
    
    <item>
      <title>CPU Cache False Sharing</title>
      <link>https://zhuyie.github.io/posts/cpu-cache-false-sharing/</link>
      <pubDate>Mon, 03 May 2021 13:06:16 +0800</pubDate>
      
      <guid>https://zhuyie.github.io/posts/cpu-cache-false-sharing/</guid>
      <description>起因 一个通过蒙特卡洛方法估算 π 值的测试程序，采用了多线程的方式，其代码大致如下：
#include &amp;#34;...&amp;#34; class RandomNumber { mt19937 mt19937_; // random number generator  const float MT19937_FLOAT_MULTI = 2.3283064365386962890625e-10f; // (2^32-1)^-1 public: void seed(unsigned int seed) { mt19937_.seed(seed); } float operator() () { return mt19937_() * MT19937_FLOAT_MULTI; } }; void worker(int worker_id, int64_t samples, RandomNumber *rnd, int64_t *in) { rnd-&amp;gt;seed(worker_id); // seed the prng  for (int64_t i = 0; i &amp;lt; samples; i++) { float x = (*rnd)(); float y = (*rnd)(); if (x*x + y*y &amp;lt;= 1) { (*in)++; } } } int main(int argc, char* argv[]) { int num_threads = 16; int64_t num_samples = 1000000000; // parsing command line arguments  auto start = system_clock::now(); RandomNumber* rnd = new RandomNumber[num_threads]; int64_t* in_points = new int64_t[num_threads]; int64_t total_points = num_samples; int64_t circle_points = 0; vector&amp;lt;thread&amp;gt; threads; for (int i = 0; i &amp;lt; num_threads; i++) { int64_t samples = num_samples / num_threads; if (i == num_threads - 1) { samples += num_samples - samples * num_threads; } threads.</description>
    </item>
    
    <item>
      <title>在MacOS命令行程序中使用Metal</title>
      <link>https://zhuyie.github.io/posts/use-metal-in-command-line-app/</link>
      <pubDate>Mon, 19 Apr 2021 21:20:23 +0800</pubDate>
      
      <guid>https://zhuyie.github.io/posts/use-metal-in-command-line-app/</guid>
      <description>出错 最近在试验用GPU做计算，由于日常工作在MacBook Pro上，很自然想要试下Apple的Metal API。
官方有个示例程序Performing Calculations on a GPU，很容易懂。下载示例代码到本地构建运行，完全正常。这么简单的吗？
那就开始用到自己的程序中吧！因为要与CPU/OpenCL等版本进行横向对比，很自然的Metal版本也使用了command line app的形式。代码写完编译很顺利，试跑一下，结果不对。。。
作为老程序员，遇到这种事情很淡定。即使是测试用的小程序，我也加上了错误处理相关的代码（良好的编程素养！），那看一下错误输出，查一下文档，一般都能搞定。
出错的代码位置是：
MTLCompileOptions* compileOptions = [MTLCompileOptions new]; id&amp;lt;MTLLibrary&amp;gt; lib = [device newLibraryWithSource:source options:compileOptions error:&amp;amp;error]; if (lib == nil) { NSLog(@&amp;#34;Failed to create library: %@.&amp;#34;, error); return nil; } 而对应的错误输出为：
2021-04-19 22:00:10.546 estimate_pi_metal[3334:87426] Failed to create library: (null). 也就是：newLibraryWithSource执行失败返回了nil，与此同时error也为null。
官方文档 newLibraryWithSource官方文档上对Return Value的说明：
A new library object that contains the compiled source code or nil if an error occurred. 对error参数的说明：</description>
    </item>
    
    <item>
      <title>VC2019 std::generate_canonical 性能问题</title>
      <link>https://zhuyie.github.io/posts/cpp-generate-canonical-perf/</link>
      <pubDate>Mon, 19 Apr 2021 11:07:20 +0800</pubDate>
      
      <guid>https://zhuyie.github.io/posts/cpp-generate-canonical-perf/</guid>
      <description>起因 最近一个项目中需要生成一堆均匀分布的伪随机浮点数。在C++11中，随机数生成算法以及相关的工具函数进行了不小的扩充。对于我的需求，常规的实现方法是：
#include &amp;lt;random&amp;gt;#include &amp;lt;iostream&amp;gt;int main() { std::random_device rd; std::mt19937 gen(rd()); // Standard mersenne_twister_engine seeded with rd()  std::uniform_real_distribution&amp;lt;float&amp;gt; dis(1.0f, 2.0f); for (int n = 0; n &amp;lt; 10; ++n) { // Use dis to transform the random unsigned int generated by gen into a  // float in [1, 2). Each call to dis(gen) generates a new random float  std::cout &amp;lt;&amp;lt; dis(gen) &amp;lt;&amp;lt; &amp;#39; &amp;#39;; } std::cout &amp;lt;&amp;lt; &amp;#39;\n&amp;#39;; return 0; } 其输出类似于：</description>
    </item>
    
  </channel>
</rss>
